#!/usr/bin/env python3
"""
Regression Model using Machine Learning
Original implementation matching the user's provided code
"""

import os # Used for the safety of the ML Model 
import pickle # To save the ML Model 

# Used for manipulating the data
import pandas as pd
import numpy as np 

# Mainly for enhancing the appearance of the data sets
import matplotlib.pyplot as plt 
import seaborn as sns

# ML Tool Kit for the project (SciKit-learn)
# This is a model that is used for predicting the numbers
from sklearn.ensemble import RandomForestRegressor

# Used for filling the data that is missing
from sklearn.impute import SimpleImputer

# Spiltting the data into sub sections for training
from sklearn.model_selection import train_test_split

# Model Performance (Trying to evaluate the performances)
from sklearn.metrics import r2_score, mean_squared_error

# This is used for normalizing the data
from sklearn.preprocessing import StandardScaler

# This takes care of both the preprocessing and the training
from sklearn.pipeline import Pipeline


def main():
    """Main function to run the regression model analysis."""
    
    country_data = "data/country_digital_features.csv" # A file from the AIA (demo file)

    # Code to execute if the path does not exist using the os (imported)
    if (os.path.exists(country_data)): 
        df = pd.read_csv(country_data)
    else: 
        raise FileNotFoundError(f"{country_data} is not found ")

    # Printing the data there
    print("The data file has been successfully loaded and ready to process........")
    print(df.head())

    # Second Step of the Project - Processing 
    columns = ['InternetPenetration', 'BroadbandSpeed', 'GDPperCapita',
                'ElectricityAccess', 'UrbanPopulation', 'MobileSubscriptions',
                'EduIndex', 'CSGraduatesPerCapita'] # columns from the data set made by the AIA (AI Assistant)

    predicted = 'WebPagesPerMillion' # Predicting variable from the data set

    # Independent and Dependent Variables in the regression model
    X = df[columns]
    y = df[predicted]

    # isnull() is a method that is used for checking if there are any missing values in the data set. 
    # .sum()counts all the missing values 
    print("\nMissing values:\n", X.isnull().sum())

    # More Processing of the data using the pipeline (Chaining the pre-processing steps)
    pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="mean")),  
        ("scaler", StandardScaler()),                    
        ("model", RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=5))            
    ]) 

    # This is used for spiltting the data into 20 percent test and 80 percent training 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Printing the statement on how many models are being tested and how many samples are being trained
    print(f"\nðŸ“Š Training on {len(X_train)} samples, testing on {len(X_test)}") # Code from the codegeek

    # The model is successfully trained
    pipeline.fit(X_train, y_train)
    print("\nThe model is successfully trained!!!")

    predictions = pipeline.predict(X_test)

    # This process is referred to as evaluating the the model that trained and tested the data set
    r2 = r2_score(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)

    print(f"The performance of the model: ")

    # R squared is the proportion of the variation in the dependent variable that is predictable from the independent variable.
    print(f"The RÂ² Score: {r2:.4f}")

    # Mean Squared Error (MSE) is a measure of the average squared difference between predicted and actual values
    print(f"The Mean Squared Error: {mse:.2f}")

    # Accessing the each step in the model essentially
    model = pipeline.named_steps["model"] # Extracted from the AIA based upon the data set 
    importances = model.feature_importances_ # Extracted from the AIA based upon the data set 

    plt.figure(figsize=(10, 5))
    sns.barplot(x=importances, y=columns) # Suggested bar chart - by the AIA 

    # The characteristics of the graph 
    plt.title("What actually drives Web Presence")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    
    # Create plots directory if it doesn't exist
    os.makedirs("plots", exist_ok=True)
    plt.savefig("plots/feature_importance.png") # image generated by the AIA (not a real png essentially)

    # Displays the graph and we could essentially interepret and draw conclusions from that graph 
    plt.show()
    
    # Save the model for later use
    os.makedirs("models", exist_ok=True)
    with open("models/regression_model.pkl", "wb") as f:
        pickle.dump(pipeline, f)
    
    print("\nâœ… Model saved to models/regression_model.pkl")
    print("ðŸ“Š Feature importance plot saved to plots/feature_importance.png")


if __name__ == "__main__":
    main()
